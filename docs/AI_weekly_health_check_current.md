# Current Health Check State & Log

## Maintenance Log

Keep a log of when this check was run and major findings:

| Date | Health Status | Critical Issues | Actions Taken | Notes |
|------|---------------|-----------------|---------------|-------|
| 2026-02-22 (afternoon) | Good | 0 | 2 | **Overall**: ðŸŸ¡ **GOOD** (0 Critical, 0 Major, 4 Minor). **Actions Taken**: 1) Deleted stuck job `kube-system/descheduler-29528880` (8h running); 2) Fixed `absenty` Flux Kustomization failures by downgrading `ImagePolicy` manifests to `v1beta2` (API `v1` not supported); 3) Verified "hardware errors" on node 12 are transient DNS timeouts from Feb 19th (false positive). **Health Check**: Nodes healthy, storage healthy, monitoring operational. **Minor Issues**: 1) Talos version drift; 2) 1 Zigbee device offline; 3) music_assistant duplicates; 4) Soil Sensor battery monitoring. **Status**: Cluster healthy, minor issues resolved. |
| 2026-02-22 | Good | 0 | 8 | **Overall**: ðŸŸ¡ **GOOD** â€” Major upgrade + maintenance session. **Patch updates**: authentik 2025.12.3â†’4, vaultwarden 1.35.2â†’3, adguard-home v0.107.71â†’72, node-red 4.1.4â†’5. **Minor updates**: descheduler chart 0.34.0â†’0.35.0, intel-device-plugin charts 0.34.1â†’0.35.0 (Ã—2), unpoller v2.33.0â†’v2.34.0, penpot chart 0.33.0â†’0.35.0 â€” all services confirmed running post-upgrade. **open-webui 10.2.1â†’12.3.0**: Required deleting immutable `open-webui-pipelines` Deployment (chart 12.x changed label selectors), adding `upgrade.timeout: 15m`, explicit Redis URL `redis://open-webui-redis:6379/0`. All pods running post-upgrade. **Flux CRD API migration**: 47 files migrated from deprecated `helm.toolkit.fluxcd.io/v2beta1/v2beta2` â†’ `/v2` and `source/image.toolkit.fluxcd.io/v1beta2` â†’ `/v1` (stable). **Renovate ignoreDeps**: Added `nachtschatt3n/k8s-self-ai-ops` (private GitHub repo, no token access). **ECK operator 2.14.0â†’3.3.0**: No breaking changes for ES/Kibana 8.15.3 users. Upgrade succeeded immediately. **fluent-bit chunk retry backlog**: Diagnosed pre-existing issue â€” 3600+ queued retry tasks, only 2 workers, 15-retry limit. ES healthy (0 write rejections, 30% heap, logs still ingesting). Fixed by restarting DaemonSet to clear backlog. Added log pipeline monitoring to health check docs. **Deferred (require investigation)**: longhorn 1.10.1â†’1.11.0 (pre-upgrade volume checks needed), cilium (HA mDNS/UPnP post-validation needed), nextcloud chart 6.6.10â†’8.9.1, uptime-kuma 2.25.0â†’4.0.0, Talos/K8s cluster upgrade, n8n v2 migration (BLOCKED), mariadb chart migration (BLOCKED). |
| 2026-02-21 (evening) | Good | 0 | 3 | **Overall**: ðŸŸ¡ **GOOD** (0 Critical, 0 Major, 5 Minor/Info). **Fixes applied this session**: 1) `monitoring/elasticsearch` ILM bootstrap Job deleted â†’ Flux recreated with curl:8.18.0, fluent-bit and kibana unblocked âœ…; 2) `office/nextcloud-notify-push` binary path corrected (`notify_push/notify_push/bin/x86_64/`) + execute bit restored â†’ pod Running 0 restarts âœ…; 3) `databases/redisinsight` rollingUpdate conflict confirmed resolved (PRs #88+#89, RollingUpdate maxSurge:0) âœ…. **Full 39-section health check**: Cluster: All 3 nodes healthy (v1.34.0, CPU ~6%, Memory 26-37%), 0 OOM kills, 0 evictions. GitOps: 85/85 kustomizations reconciled, 72/72 HelmReleases ready, 46/46 HelmRepositories ready, 7/7 Flux controllers running (47d uptime, 0 restarts). Storage: 68/68 Longhorn volumes attached/healthy, autoDeletePod=false âœ…, 75/75 PVCs Bound. Backups: daily-backup-all-volumes completed 13h ago âœ…. Certs: 8/8 Ready, renewing mid-March. DaemonSets: 12/12 healthy. Pods: 0 CrashLoopBackOff, 0 Pending, 0 Terminating. Container log errors: 0 (Cilium, CoreDNS, Flux, cert-manager). Network: UnPoller healthy (Err:0, 104-106 clients, 1 USG+5 USW+4 UAP); MQTT: 0 auth failures, 0 errors. Home Automation: HA running 6d18h, Zigbee 22/23 seen today (1 offline >7d: 0x00124b002d12beec - likely dead battery), Mosquitto healthy, Frigate running 12d 0 restarts. Databases: All 8 pods running. Media: Jellyfin 14d, Plex 41d, JDownloader 7h all running. Ingress: 0 controller errors. Webhooks: 7 validating + 4 mutating, 0 failures. **Minor/Info**: external-dns 29 restarts (diagnosed: transient Cloudflare EOF, self-healing, no action needed âœ…); unpoller 11 restarts (diagnosed: healthy, Err:0, periodic re-auth normal âœ…); Talos version drift client v1.11.6 vs nodes v1.11.0 (upgrade needed); music_assistant duplicate entity IDs in HA (non-critical); cloudflared 5 restarts 2d15h ago (monitoring). **Talos client/node version mismatch**: client v1.11.6, nodes v1.11.0 â€” upgrade nodes when convenient. |
| 2026-02-21 | Warning | 1 | 3 | **Overall**: ðŸŸ  **WARNING** (1 real Critical - opencode OOM, 0 Major, 1 Minor - node 12 noise). **Cluster**: All 3 nodes healthy (v1.34.0, CPU 4-5%, Memory 26-38%), 0 warning events, 3 OOM kills (opencode), 0 pod evictions. **GitOps**: 83/83 kustomizations reconciled, 72/72 HelmReleases ready, all HelmRepositories healthy, 7/7 Flux controllers running. **Storage**: 58/58 Longhorn volumes healthy, autoDeletePod=false, 75 PVCs all Bound. **Backups**: daily-backup-all-volumes completed successfully (12h ago, 17m duration), 5/5 recent jobs successful. **Certificates**: 8/8 ready. **DaemonSets**: All healthy. **Monitoring**: Prometheus & Alertmanager running, 0 firing alerts. **Pods**: 0 CrashLoopBackOff, 0 Pending; 2 pods with elevated restarts (opencode-andreamosteller: 1 - recent OOM, external-dns: 28 - historical). **Network**: UnPoller healthy (10 devices, 115 clients, 0 export errors), MQTT broker healthy (46/54 clients connected, 15 Shelly devices, 0 auth failures, 0 errors). **Home Automation**: HA running (errors: Tibber 4403 Invalid Token - FALSE POSITIVE external bug, music_assistant duplicate entity IDs), Zigbee2MQTT healthy (23 devices, 0 errors), Mosquitto healthy. **Frigate**: All 6 cameras streaming, MQTT availability online, 0 crash loops. **Batteries**: 20 battery devices (avg 81%, 0 critical, 0 warning, 1 monitor: Soil Sensor 1 at 42%). **Elasticsearch Logs**: 10,000+ errors/day (fluent-bit: 40K, kube-apiserver: 11K). 3 FATAL/OOM errors: opencode OOMKilled, external-dns transient EOF. **FIXED**: Increased opencode memory limit to 2Gi; Confirmed Tibber 4403 is external platform bug. |
| 2026-02-15 | Good | 0 | 1 | **Overall**: ðŸŸ¡ **GOOD** (0 real Critical, 0 Major, 2 Minor). **Cluster**: All 3 nodes healthy (v1.34.0, CPU 4-7%, Memory 29-39%), 0 warning events, 0 OOM kills, 0 pod evictions. **GitOps**: 83/83 kustomizations reconciled, 72/72 HelmReleases ready, all HelmRepositories healthy, 7/7 Flux controllers running. **Storage**: 58/58 Longhorn volumes healthy, autoDeletePod=false, 74 PVCs all Bound. **Backups**: daily-backup-all-volumes completed successfully (8h ago, 22m duration), 5/5 recent jobs successful. **Certificates**: 8/8 ready. **DaemonSets**: All healthy. **Monitoring**: Prometheus & Alertmanager running, 0 firing alerts. **Pods**: 0 CrashLoopBackOff, 0 Pending; 2 pods with elevated restarts (unpoller: 9, external-dns: 13 - historical). **Network**: UnPoller healthy (10 devices, 109 clients, 0 export errors), MQTT broker healthy (46/54 clients connected, 15 Shelly devices in recent logs, 0 auth failures, 0 errors). **Home Automation**: HA running (31 major errors - mostly music_assistant duplicate entity IDs, samsung_familyhub_fridge API error, dynamic_energy_cost unavailable, ESPHome voice device unreachable), Zigbee2MQTT healthy (23 devices, 0 errors), Mosquitto healthy. **Frigate**: All 6 cameras streaming, MQTT availability online, 0 crash loops. **Batteries**: 20 battery devices (avg 82%, 0 critical, 0 warning, 1 monitor: Soil Sensor 1 at 46%). **Elasticsearch Logs**: 10,000+ errors/day (Frigate: 36K, fluent-bit: 29K, kube-apiserver: 7.8K). 1 FATAL error: external-dns transient EOF (self-recovered). **FIXED**: Removed 2 duplicate `kids_room_ventilate_reminder` automations from HA config. **ShellyWallDisplay-000822A9320E** (WallDisplay Upper Hallway, Shelly Wall Display, upper_hallway): Still rapid MQTT reconnection cycling every ~5 seconds. **FALSE POSITIVE DOCUMENTED**: Authentik outpost ExternalName services showing "no backends" is expected - ExternalName services resolve via DNS, not Endpoints objects. Updated health-check.sh to skip ExternalName services and AI_weekly_health_check.MD to document this. **Ingress**: 87 errors at check time but cleared shortly after. **Webhooks**: 11 configured, all healthy. |
| 2026-02-12 | Good | 0 | 0 | **Overall**: ðŸŸ¡ **GOOD** (0 Critical, 2 Major, 2 Minor). **Cluster**: All 3 nodes healthy (v1.34.0, CPU 4-6%, Memory 27-37%), 0 warning events, 0 OOM kills, 0 pod evictions. **GitOps**: 82/82 kustomizations reconciled, 72/72 HelmReleases ready, all HelmRepositories healthy, 7/7 Flux controllers running. **Storage**: 58/58 Longhorn volumes healthy, autoDeletePod=false, 74 PVCs all Bound. **Backups**: daily-backup-all-volumes completed successfully (12h ago, 17m duration), 4/5 recent jobs successful. **Certificates**: 8/8 ready. **DaemonSets**: All healthy. **Monitoring**: Prometheus & Alertmanager running, 1 firing alert (LonghornVolumeUsageWarning: influxdb2-data-10g). **Pods**: 0 CrashLoopBackOff, 0 Pending; 2 pods with elevated restarts (clawd-bot: 82, external-dns: 6 - historical). **Network**: UnPoller healthy (10 devices, 105-106 clients, 0 export errors), MQTT broker healthy (46/54 clients connected, 26 Shelly devices, 0 auth failures, 0 errors). **Home Automation**: HA running (95 major errors - almost entirely Bermuda metadevice bugs from Entry SW02 + Tesla Wall Connector timeouts + 1 Shelly reconnection error), Zigbee2MQTT healthy (23 devices, 0 errors), Mosquitto healthy. **Batteries**: All 20 battery devices healthy (avg 82%, 0 critical, 0 warning). **Elasticsearch Logs**: 10,000 errors/day (mostly fluent-bit self-referential logs in monitoring namespace: 39,075; kube-apiserver: 10,510; home-automation: 3,096 - no FATAL/OOM errors). **ShellyWallDisplay-000822A9320E**: Still rapid reconnection cycling every ~5 seconds from 192.168.33.47. **Ingress**: 11 Authentik outpost services showing no backends (ExternalName services, expected behavior). **Webhooks**: 11 configured (7 validating, 4 mutating), all healthy. **Minor Issues**: 1) talosctl unavailable; 2) Elevated log error count: 10,000. |
| 2026-02-09 | Excellent | 0 | 0 | **Overall**: âœ… **EXCELLENT** (0 Critical, 0 Major, 4 Minor). **Cluster**: All 3 nodes healthy (v1.34.0, CPU 5-7%, Memory 27-34%), 0 warning events, 0 OOM kills, 0 pod evictions. **GitOps**: 82/82 kustomizations reconciled, 72/72 HelmReleases ready, all HelmRepositories healthy, 7/7 Flux controllers running. **Storage**: 58/58 Longhorn volumes healthy, autoDeletePod=false, 0 PVC issues. **Backups**: daily-backup-all-volumes completed successfully (11h ago, 18m duration), 4/5 recent jobs successful. **Certificates**: 8/8 ready. **DaemonSets**: All healthy. **Monitoring**: Prometheus & Alertmanager running, 0 firing alerts. **Pods**: 0 CrashLoopBackOff, 0 Pending; 2 pods with elevated restarts (clawd-bot: 22, external-dns: 6 - historical). **Network**: UnPoller healthy (10 devices, 104-105 clients, 0 export errors), MQTT broker healthy (46/54 clients connected, 0 auth failures, 0 errors). **Home Automation**: HA running (10 errors - external integrations: TeslaFi read errors, Shelly Entry Window Blinds, Dirigera hub disconnects, Wyze camera API 504, Tesla Wall Connector timeouts), Zigbee2MQTT healthy (23 devices, 0 errors), Mosquitto healthy. **Batteries**: All 20 battery devices healthy (avg 82%, 0 critical, 0 warning). **Elasticsearch Logs**: 10,000 errors/day (mostly fluent-bit self-referential logs in monitoring namespace: 36,914; kube-apiserver: 9,777; home-automation: 1,433 - no FATAL/OOM errors). **ShellyWallDisplay-000822A9320E**: Rapid reconnection cycling every ~5 seconds from 192.168.33.47 (investigate keepalive/firmware). **Minor Issues**: 1) talosctl unavailable; 2) HA integration errors: 10; 3) Elevated log error count: 10,000; 4) UnPoller minor errors: 1. |
| 2026-02-06 | Good | 0 | 0 | **Overall**: âœ… **GOOD** (0 Critical, 2 Major, 2 Minor). **Major Issues**: 1) scrypted HelmRelease failed (install timeout - pod running 25d, stale failure state); 2) Flic Hub offline at 192.168.33.133 (not reachable, HA connection errors). **Minor Issues**: 1) Prometheus & InfluxDB volume alerts (high usage warnings, but under 75%); 2) Home Assistant errors: 371 (mostly Bermuda metadevice bugs, Flic Hub, Tibber token invalid). **Positive**: All 3 nodes healthy (v1.34.0), all pods running (181/199 Running), all 8 certificates ready, 82/82 kustomizations ready, 71/72 HelmReleases ready, 58 Longhorn volumes healthy, backup successful (6h ago), DaemonSets healthy, no hardware errors, no pod evictions, Zigbee batteries all healthy (lowest 63%), teslamate resolved & running, zigbee2mqtt MQTT connected. **Status**: Cluster healthy, investigate Flic Hub offline, Tibber token needs refresh. |
| 2026-01-26 | Excellent | 0 | 4 | **Overall**: âœ… **EXCELLENT** (0 Critical, 0 Major, 0 Minor). **MAJOR SUCCESS**: Shelly MQTT infrastructure completely resolved! **Accomplishments**: 1) Fixed Cilium v1.18.6 LoadBalancer issue (restarted DaemonSet, restored connectivity to MQTT broker at 192.168.55.15); 2) Comprehensive Shelly device audit completed (scanned 192.168.32.0/23, found 39 physical devices vs 74 cloud entries); 3) Fixed 4 Shelly devices: 3 Wall Displays (192.168.33.42, 43, 47) + Fridge (192.168.32.149) - all configured with MQTT credentials; 4) Added Section 22a to health check documentation and script for ongoing MQTT/Shelly monitoring. **Results**: 38/39 devices connected to MQTT (97.4% success rate), only 1 device offline (Dining Room wall display confirmed offline in cloud). **Mystery Solved**: "36 missing devices" explained - 28 virtual i4 inputs, 10 battery sensors (sleeping), 2 server VLAN devices, 4 actually offline, 1 virtual thermostat. **Documentation**: Updated AI_weekly_health_check.MD (35 sections), health-check.sh script, comprehensive reports generated. **Status**: All critical systems operational, smart home infrastructure healthy, monitoring enhanced. |
| 2026-01-25 00:18 | Warning | 1 | 2 | **Overall**: ðŸŸ  WARNING (1 Critical, 2 Major, 2 Minor). **Critical Issues**: 1) teslamate pod in CrashLoopBackOff (MQTT connection timeout - config fixed to mosquitto-main, investigating network connectivity). **Major Issues**: 1) teslamate deployment 0/1 replicas (same as critical - MQTT connection issue); 2) Critical batteries: 2 devices <15% (Soil sensor 3: 2%, Soil Sensor 2: 14% - immediate replacement needed). **Minor Issues**: 14 warning events (mostly normal operations), UnPoller 2 errors (minor). **FIXED**: zigbee2mqtt MQTT connection resolved (service name updated to mosquitto-main, pod recreated successfully, web interface accessible); zigbee2mqtt 502 Bad Gateway resolved (pod restarted, MQTT connected). **Positive**: All Prometheus alerts cleared (0 firing), all certificates ready, all DaemonSets healthy, GitOps fully synchronized, no OOM kills or pod evictions, storage healthy, Elasticsearch logs: 0 errors today (excellent!), hardware health excellent (0 errors on all nodes). **Status**: Cluster stable, zigbee2mqtt working, teslamate MQTT connection needs investigation, battery replacement urgent. |
| 2026-01-24 Night | Warning | 0 | 0 | **Overall**: ðŸŸ  WARNING (0 Critical, 4 Major, 6 Minor). **Major Issues**: 1) Nextcloud HelmRelease in "Unknown" state (upgrade in progress - transient); 2) Nextcloud deployment 0/1 replicas (upgrade in progress); 3) Home Assistant errors: 56 (mostly external integrations - Tesla Wall Connector timeouts expected, Dirigera hub disconnects, Deebot command timeouts, Bermuda metadevice warnings); 4) Critical battery: Soil sensor 3 at 2% (immediate replacement needed). **Minor Issues**: 15 warning events (mostly normal operations), 1 terminating pod (Nextcloud upgrade), Zigbee2MQTT 50 warnings (normal operation), 1 low battery (Soil Sensor 2 at 15%), 10,000 log errors (mostly Frigate/Zigbee2MQTT - normal), UnPoller 4 errors. **Positive**: All certificates ready (8/8), backup system operational (last backup 17h ago), all DaemonSets healthy, GitOps 81/81 reconciled, no OOM kills, no pod evictions, hardware health excellent (0 errors on all nodes), storage healthy. **Status**: Cluster stable, Nextcloud upgrade in progress (transient), battery replacement urgent. |
| 2026-01-18 Night | Good | 0 | 2 | **RESOLVED**: Longhorn volume alerts completely cleared (PVC expansion 20Giâ†’40Gi successful); **Investigated**: Home Assistant errors (87 total) - **1)** Uptime Kuma authentication failure (needs API token config); **2)** 86 duplicate sensor IDs (multiple monitors with same names); **Hardware errors clarified**: "Errors" are benign iSCSI virtual disk messages, not real hardware failures (normal for Longhorn operations). **Status**: Storage crisis resolved, HA integration issues identified for future fix. |
| 2026-01-17 Night | Good | 0 | 4 | **Investigated & Actioned**: **1)** Resized `clawd-bot-data` PVC to 20Gi (was 98% full); **2)** Investigated Node 12 Hardware errors: `bio_check_eod` likely related to iSCSI/Longhorn volumes (many virtual disks present) rather than physical NVMe failure; **3)** Confirmed Home Assistant Tesla error is a code bug in v2.13.0 (`KeyError: 'None'` on shifter state); **4)** Confirmed UnPoller metrics empty due to InfluxDB usage. **Status**: Cluster healthy, maintenance items identified. |
| 2026-01-17 PM | Warning | 0 | 3 | **Resolved**: Health check script fixed (jq execution moved to host, syntax errors fixed). **New Findings**: 2 Low Battery Zigbee devices (9%, 16%); Hardware errors on Node 12 identified as `bio_check_eod` (disk/IO errors); UnPoller metrics empty (InfluxDB confirmed). **Status**: Cluster stable, but hardware and batteries need attention. |
| 2026-01-17 | Warning | 1 | 2 | **Mixed**: UnPoller service fixed (metrics target UP in Prometheus, but metrics empty - user switched to InfluxDB); Pod evictions cleared (clawd-bot npm-cache limit increased); New Service created for UnPoller to fix missing target; **Known Issue**: UnPoller metrics checks failing due to InfluxDB switch; **Critical**: 2 Pod evictions detected (resolved by limit increase); Home Assistant errors: 30 (external integrations); Backup system healthy |
| 2026-01-10 PM | Excellent | 0 | 2 | **MAJOR IMPROVEMENTS**: Certificate conflict RESOLVED (adguard-home-tls now Ready via cert-manager ingress annotation); tube-archivist PVC reconciliation RESOLVED (manifest updated to 12Gi, kustomization Ready); All certificates Ready (5/5 = 100%); Backup system working (last backup 8h ago, 44/45 volumes backed up); All Prometheus alerts cleared (only Watchdog); **NEW ISSUE**: clawd-bot kustomization failing (missing secret.sops.yaml); clawd-bot-data volume detached but PVC bound; Home Assistant errors increased to 40/100 lines; Zigbee devices: 22 total (battery check needs investigation) |
| 2026-01-10 AM | Good | 1 | 0 | **EXCELLENT**: All Prometheus alerts cleared (0 firing); Home Assistant errors DOWN to 1 (from 2!); Backup system working (last backup 6h47m ago, completed successfully); All 3 nodes healthy; GitOps 59/60 reconciled (1 issue: tube-archivist PVC - manifest still shows 10Gi, needs update to 12Gi); Certificate conflict: adguard-home-tls STILL EXISTS (duplicate certificate not resolved); **NEW**: 1 unhealthy volume (clawd-bot-data: detached, unknown robustness - new volume created since yesterday); **CRITICAL**: Zigbee batteries still need replacement (12%, 18% - unchanged) |
| 2026-01-09 PM | Good | 1 | 0 | **EXCELLENT**: All Prometheus alerts cleared (0 firing); Home Assistant errors DOWN to 2 (from 11!); external-dns stable (813 restarts historical, now running fine); All 44 volumes healthy; All 3 nodes healthy; GitOps 59/60 reconciled (1 issue: tube-archivist PVC - manifest shows 10Gi but PVC is 12Gi, needs manifest update); Certificate conflict: adguard-home-tls (Helm chart creating duplicate certificate, needs disabling); Backup system: **WORKING** - Longhorn RecurringJob running successfully (last backup 17h ago, job completed); **CRITICAL**: Zigbee batteries still need replacement (12%, 18%) |
| 2026-01-09 PM | Good | 1 | 3 | FIXED: external-dns stabilized (penpot annotation added); tube-archivist volume expanded 10Giâ†’12Gi (94.3%â†’78.8%); adguard-home-tls certificate fixed (removed duplicate annotation); All Prometheus alerts cleared (except Watchdog) | CRITICAL: Zigbee batteries still need replacement (12%, 18%); Tesla Wall Connector timeouts are EXPECTED (power save mode when not charging); All services operational; external-dns running stable with 0 errors; Volume expansion successful; Certificate conflict resolved |
| 2026-01-09 AM | Warning | 1 | 6 | CRITICAL: Zigbee batteries deteriorating further (12%, 18% - down from 14%, 20%); 5 Prometheus alerts firing; Tesla Wall Connector regression (4 timeouts) | MAJOR IMPROVEMENTS: Backup system RESTORED (last backup 13h ago); Node 3 UNCORDONED and healthy; Home Assistant errors DOWN to 11 (from 93); Amazon Alexa RESOLVED (0 failures); All 44 volumes healthy; All 3 nodes at 5% CPU; GitOps synchronized; Cloudflare tunnel operational; Battery average: 80% (stable); WARNING: adguard-home-tls certificate not ready; external-dns CrashLoopBackOff is EXPECTED (Cloudflare proxy rejects private IPs, external access via tunnel works fine) |
| 2026-01-06 | Warning | 2 | 6 | CRITICAL: Backup system completely broken (0 backup jobs, 0 volumes backed up); Zigbee battery crisis (2 devices <20%, 14% and 20%); Home Assistant integration issues (15 errors, 6 Amazon Alexa failures); Jellyfin health check failed; Database connectivity issues | Node 3: SSD detected but DEFECTIVE - cordoned, monitoring removed, replacement ordered (arrives in 2 days); 33 hardware errors on node 11 (investigation needed); Zigbee devices: 22 total, 17 battery-powered; Battery average: 81%; 2 CRITICAL batteries: 14%, 20% (immediate replacement required); Home Assistant: 15 errors/100 lines; Amazon Alexa: 6 failures; Tesla Wall Connector: 0 timeouts (resolved); All infrastructure stable: 0 events, 0 crashes, 53/53 volumes healthy; GitOps perfect; Network healthy; DNS working; External access functional |
| 2026-01-01 | Excellent | 0 | 2 | Prometheus volume alerts resolved with filesystem trim; recurring trim job configured | Resolved Longhorn actualSize metric false positives (100.1% â†’ 6.5%); Created prometheus-filesystem-trim recurring job (daily 2 AM); Manual trim reclaimed 93.6 GiB; All 3 alerts cleared; Samsung 990 PRO SSD warranty claim package prepared (Node 3 defective drive); Trim job should be monitored for effectiveness |
| 2025-12-31 PM | Good | 0 | 4 | MAJOR: Backup failure investigation and resolution (149 alerts cleared) | Investigated massive backup failure (147 failed backups); Root cause: Network/CIFS performance bottleneck (NAS healthy, 30 MB/s observed vs 200+ MB/s expected on 10 GbE); Cleared 147 failed backup CRs; Backup speeds varied 10x (5.6-57.5 GB/min); 51 backups successful; Network path investigation needed |
| 2025-12-31 AM | Good | 0 | 1 | Node 3 uncordoned after successful validation; Prometheus volume fix applied | Node 3 uncordoned (8+ days stable); Prometheus alerts firing (false positive - snapshot deletion in progress); Battery health unchanged (2 critical: 15%, 21%); All applications healthy; Flux reconciled |
| 2025-12-30 | Good | 0 | 1 | Day 8/9 of Node 3 SSD monitoring - validation period exceeded, ready to uncordon | Node 3 SSD health: EXCELLENT (34Â°C, 100% spare, 0 errors); Battery health stable (avg 82%, 2 critical devices deteriorating: 15%, 21%); Amazon Alexa integration still failing (40 failures); Tesla Wall Connector: 2 timeouts reappeared; IKEA Dirigera: 1 listener failure |
| 2025-12-28 | Good | 0 | 0 | Day 6 of 7 Node 3 SSD validation - all excellent | Node 3 SSD health: PASSED (34Â°C, 100% spare, 0 errors); Battery health improved (avg 79%); Amazon Alexa integration failing |
| 2025-12-26 | Good | 0 | 2 | Fixed Jellyfin health check parsing bug | Corrected health check script to handle plain text response instead of JSON; updated current health status |
| 2025-12-25 | Good | 0 | 3 | Updated health check with latest investigation results | Resolved paperless redis replicas and jellyfin health endpoint issues; added node 3 SSD monitoring details |
| 2025-12-13 | Excellent | 0 | 1 | Major expansion: Added 9 new health check sections for comprehensive home lab monitoring | Added home automation, media services, database health, external services, security monitoring, performance trends, backup verification, environmental monitoring, and application-specific checks |
| 2025-12-13 | Excellent | 0 | 0 | Updated UniFi network section with enhanced event log checking | Added checks for WAN/Internet disconnects, client errors, device issues, and security events; clarified system unifictl usage |
| 2025-11-27 | Excellent | 0 | Updated health check documentation with command reference | Added tested commands and common pitfalls section |
| 2025-11-15 | Excellent | 0 | Fixed pgadmin cert, cleaned orphaned volumes | All systems operational |

---

## Current Health Check Report

```markdown
# Kubernetes Cluster Health Check Report
**Date**: 2026-02-22 CET
**Cluster**: cberg-home-nextgen
**Nodes**: 3 (k8s-nuc14-01, k8s-nuc14-02, k8s-nuc14-03)
**Kubernetes Version**: v1.34.0

## Executive Summary
- **Overall Health**: ðŸŸ¡ **GOOD**
- **Critical Issues**: 0
- **Major Issues**: 0
- **Minor Issues**: 4 (Talos version drift, 1 Zigbee device offline, music_assistant dupes, Soil Sensor battery)
- **Service Availability**: 99%+
- **Node Status**: âœ… ALL 3 NODES HEALTHY

## Upgrades Applied This Session (2026-02-22)
| Component | From | To | Notes |
|-----------|------|----|-------|
| authentik chart | 2025.12.3 | 2025.12.4 | Patch â€” running âœ… |
| vaultwarden image | 1.35.2 | 1.35.3 | Patch â€” running âœ… |
| adguard-home image | v0.107.71 | v0.107.72 | Patch â€” running âœ… |
| node-red image | 4.1.4 | 4.1.5 | Patch â€” running âœ… |
| descheduler chart | 0.34.0 | 0.35.0 | Minor â€” running âœ… |
| intel-device-plugin charts | 0.34.1 | 0.35.0 | Minor (Ã—2) â€” running âœ… |
| unpoller image | v2.33.0 | v2.34.0 | Minor â€” running âœ… |
| penpot chart | 0.33.0 | 0.35.0 | Minor â€” running âœ… |
| open-webui chart + image | 10.2.1 / 0.7.2 | 12.3.0 / 0.8.3 | Major â€” immutable Deployment deleted, explicit Redis URL added, timeout extended âœ… |
| eck-operator chart | 2.14.0 | 3.3.0 | Major â€” no breaking changes for ES/Kibana 8.15.3 âœ… |
| Flux CRD API versions | v2beta1/v2beta2 / v1beta2 | v2 / v1 | 47 files migrated to stable APIs âœ… |

## Service Availability Matrix
| Service | Internal | External | Health | Status Notes |
|---------|----------|----------|--------|--------------|
| Authentik | âœ… | âœ… | Healthy | Upgraded to 2025.12.4 |
| Home Assistant | âœ… | âœ… | Healthy | Running, Tibber 4403 FALSE POSITIVE (unchanged) |
| zigbee2mqtt | âœ… | âœ… | Healthy | 23 devices, 22/23 seen today |
| Mosquitto MQTT | âœ… | N/A | Healthy | 0 auth failures, 0 errors |
| Nextcloud | âœ… | âœ… | Healthy | Running, notify-push path fixed (prior session) |
| Jellyfin | âœ… | âœ… | Healthy | Running |
| Plex | âœ… | âœ… | Healthy | Running |
| Tube Archivist | âœ… | âœ… | Healthy | Running, elasticsearch + redis healthy |
| Grafana | âœ… | âœ… | Healthy | Running |
| Prometheus | âœ… | âœ… | Healthy | Running, 0 alerts firing |
| Alertmanager | âœ… | âœ… | Healthy | Running |
| Longhorn UI | âœ… | âœ… | Healthy | All volumes attached and healthy |
| Frigate | âœ… | âœ… | Healthy | Running, 0 restarts, 0 camera crashes |
| Backup System | âœ… | N/A | Healthy | Daily backup on schedule |
| UnPoller | âœ… | N/A | Healthy | Upgraded to v2.34.0, Err:0 |
| Elasticsearch | âœ… | N/A | Healthy | 0 write rejections, ~30% heap, green |
| fluent-bit | âœ… | N/A | Healthy | DaemonSet restarted to clear retry backlog |
| open-webui | âœ… | âœ… | Healthy | Upgraded 10.2.1â†’12.3.0 / 0.7.2â†’0.8.3 |
| ECK operator | âœ… | N/A | Healthy | Upgraded 2.14.0â†’3.3.0 |
| external-dns | âœ… | âœ… | Healthy | Self-healing Cloudflare EOF, no DNS outages |
| All databases | âœ… | N/A | Healthy | postgres, mariadb, influxdb, redis, nocodb all running |

## Detailed Findings

### 1. Talos Version Drift
ðŸ”µ **Status: MINOR** â€” talosctl client v1.11.6 vs nodes v1.11.0
- Upgrade nodes to v1.11.6 when convenient to avoid API compatibility issues.

### 2. Zigbee Device Offline >7 Days
ðŸ”µ **Status: MINOR** â€” `0x00124b002d12beec` last seen 2026-02-05
- Likely dead battery or decommissioned device. Check physically or remove from config.

### 3. Home Assistant music_assistant Duplicate Entities
ðŸ”µ **Status: MINOR/INFO** â€” non-unique entity IDs
- Affected: Downstairs, Andrea's Family Hub, Pioneer VSX-1131 (Ã—2), HA Voice media players.
- Entities are ignored by HA. No functional impact. Clean up duplicate names in music_assistant.

### 4. Battery Health
âœ… **Status: GOOD** â€” all batteries acceptable
- **CRITICAL (<30%)**: 0
- **WARNING (30-50%)**: 0 (Soil Sensor 1 was 42% last check â€” monitor)
- **Zigbee devices**: 23 total, 0 low batteries detected

### 5. Infrastructure Health
âœ… **Status: EXCELLENT**
- **Nodes**: All 3 Ready (v1.34.0), CPU ~6%, Memory 26-37%
- **Node pressure**: 0 (no DiskPressure/MemoryPressure/PIDPressure)
- **Certificates**: 8/8 ready, renewing mid-March (auto-managed)
- **DaemonSets**: 12/12 healthy
- **GitOps**: All kustomizations reconciled, all HelmReleases ready
- **Flux Controllers**: 7/7 running
- **Volumes**: All attached and healthy, autoDeletePod=false âœ…
- **PVCs**: All Bound
- **Backup**: Running on daily schedule
- **OOM Kills**: 0
- **Pod Evictions**: 0
- **CrashLoopBackOff**: 0
- **Pending pods**: 0
- **Webhooks**: All healthy
- **Ingress controller errors**: 0

### 6. Network & MQTT
âœ… **Status: HEALTHY**
- **UnPoller**: Err:0 every cycle, periodic re-auth normal
- **MQTT**: 0 auth failures, 0 connection errors
- **external-dns**: Transient Cloudflare EOF, self-heals on restart. No DNS outages.

### 7. Fluent-bit â†’ Elasticsearch Log Pipeline
ðŸ”µ **Status: MONITOR** â€” backlog cleared; watch for recurrence

**Background**: During the 2026-02-22 session, fluent-bit was found to have accumulated 3600+ chunk retry tasks with only 2 workers processing them. This built up during a prior ES restart (ILM Job fix on 2026-02-21). ES itself was healthy (0 write rejections, 30% heap). The DaemonSet was restarted to clear the retry queue. Logs were still flowing during the backlog (720K docs that day).

**What to check each session:**

```bash
# 1. Fluent-bit pod health â€” all 3 nodes should be Running
kubectl get pods -n monitoring -l app.kubernetes.io/name=fluent-bit

# 2. Fluent-bit error rate in current logs (look for retry failures)
kubectl logs -n monitoring -l app.kubernetes.io/name=fluent-bit --tail=200 \
  | grep -E "cannot be retried|failed to flush|error flushing"

# 3. Check for backlog warning (many retries queued)
kubectl logs -n monitoring -l app.kubernetes.io/name=fluent-bit --tail=100 \
  | grep -E "retry|Retry" | tail -20

# 4. Elasticsearch cluster health
kubectl exec -n monitoring elasticsearch-es-default-0 -- \
  curl -s http://localhost:9200/_cluster/health | python3 -c \
  "import sys,json; h=json.load(sys.stdin); print(f\"Status: {h['status']}, Nodes: {h['number_of_nodes']}, Shards: {h['active_shards']}\")"

# 5. ES write thread pool â€” check for rejections (should be 0)
kubectl exec -n monitoring elasticsearch-es-default-0 -- \
  curl -s 'http://localhost:9200/_cat/thread_pool/write?v&h=name,active,queue,rejected'

# 6. Today's fluent-bit index doc count (verify logs are flowing)
kubectl exec -n monitoring elasticsearch-es-default-0 -- \
  curl -s "http://localhost:9200/_cat/indices/fluent-bit-$(date +%Y.%m.%d)?v&h=index,docs.count,store.size"

# 7. ES heap usage (warn if >70%)
kubectl exec -n monitoring elasticsearch-es-default-0 -- \
  curl -s 'http://localhost:9200/_cat/nodes?v&h=name,heap.percent,heap.current,heap.max'
```

**Warning Signs (investigate if any are true):**
- fluent-bit pods not Running on all 3 nodes
- `cannot be retried` or `failed to flush` appearing in current logs (post-restart)
- ES write rejections > 0 (means ES is backpressuring; check heap/shard count)
- Today's doc count = 0 or same as yesterday (logs stopped flowing)
- ES heap > 70% (risk of GC pressure causing write rejections)
- ES status = `red` (critical: shard allocation failure)

**Recovery Procedure (if backlog recurs):**
1. First confirm ES is healthy (status green/yellow, 0 write rejections, heap < 70%)
2. If ES is healthy but fluent-bit logs show thousands of retry entries: restart the DaemonSet
   ```bash
   kubectl rollout restart daemonset/fluent-bit -n monitoring
   kubectl rollout status daemonset/fluent-bit -n monitoring
   ```
3. If ES is rejecting writes (heap > 80%): scale down non-essential workloads or force-merge old indices
4. Do NOT restart fluent-bit while ES is unhealthy â€” backlog will just rebuild

**Known False Positive**: fluent-bit logs itself (monitoring namespace) producing ~40K log entries/day that fluent-bit then tries to ship back to ES â€” creating a feedback loop in the error counts. This is normal and expected; the actual application log count is lower.

## Action Items

### ðŸŸ¡ Upgrade Backlog (Deferred â€” needs research/window)
1. **longhorn 1.10.1 â†’ 1.11.0** â€” check all 68 volumes healthy, no degraded replicas, CRD migration needed
2. **cilium 1.18.6 â†’ 1.19.1** â€” validate HA mDNS, UPnP, MQTT, ESPHome all work post-upgrade
3. **nextcloud chart 6.6.10 â†’ 8.9.1** â€” research breaking changes
4. **uptime-kuma chart 2.25.0 â†’ 4.0.0** â€” research major version breaking changes
5. **Talos nodes v1.11.0 â†’ v1.12.2 + Kubernetes v1.34.0 â†’ v1.35.0** â€” full cluster upgrade window

### ðŸ”´ Blocked (do not upgrade without resolution)
- **mariadb chart 11.5.7 â†’ 25.0.0** â€” password injection breaking change, no explicit tag support
- **n8n 1.123.x â†’ 2.9.1** â€” missing task runner in chart 1.1.0, need migration strategy

### ðŸ”µ Low Priority
1. **Check Zigbee device** `0x00124b002d12beec` â€” replace battery or remove from config
2. **Clean up music_assistant** â€” rename duplicate media player entries
3. **Monitor Soil Sensor 1 battery** â€” was 42% last check
4. **Monitor fluent-bit** â€” watch for retry backlog recurrence after DaemonSet restart

### âœ… Resolved Since Last Check (2026-02-21 evening)
1. Patch updates applied: authentik, vaultwarden, adguard-home, node-red âœ…
2. Minor updates applied: descheduler, intel-device-plugin (Ã—2), unpoller, penpot âœ…
3. open-webui upgraded 10.2.1â†’12.3.0 / 0.7.2â†’0.8.3 (with immutable Deployment fix) âœ…
4. ECK operator upgraded 2.14.0â†’3.3.0 âœ…
5. Flux CRD API versions migrated to stable (47 files) âœ…
6. Renovate ignoreDeps: added k8s-self-ai-ops private repo âœ…
7. fluent-bit retry backlog cleared (DaemonSet restarted) âœ…
8. Stuck descheduler Job deleted (8h duration) âœ…
9. Flux ImagePolicy API version fixed (`v1` â†’ `v1beta2` for absenty) âœ…

## Summary

**Overall Health**: ðŸŸ¡ **GOOD**

Major upgrade session completed: 10 components upgraded (4 patch, 5 minor, 2 major), 47 Flux CRD API files migrated to stable versions, fluent-bit retry backlog cleared. Post-maintenance cleanup resolved stuck jobs and residual Flux API deprecation issues. All services confirmed running. Remaining items are deferred upgrades requiring dedicated maintenance windows (longhorn, cilium, nextcloud, uptime-kuma, Talos/K8s cluster) and two blocked items requiring further research (mariadb, n8n).
```
