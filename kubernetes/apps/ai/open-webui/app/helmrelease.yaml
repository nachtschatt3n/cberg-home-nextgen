---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: open-webui
spec:
  interval: 30m
  chart:
    spec:
      chart: open-webui
      version: 5.13.0
      sourceRef:
        kind: HelmRepository
        name: open-webui
        namespace: flux-system
  maxHistory: 1
  install:
    remediation:
      retries: 1
  upgrade:
    cleanupOnFail: false
    remediation:
      retries: 1
  uninstall:
    keepHistory: false
  values:
    nameOverride: ""
    ollama:
      # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
      enabled: false
#       # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
#       fullnameOverride: "open-webui-ollama"
#       # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
#       ollama:
#         # gpu:
#         #   enabled: true
#         #   type: 'cpu'
#         #   number: 1
#         models:
#           pull:
#             - llama3
# #      runtimeClassName: cpu
#       persistentVolume:
#         enabled: true
#         existingClaim: open-webui-ollama

    pipelines:
      # -- Automatically install Pipelines chart to extend Open WebUI functionality using Pipelines: https://github.com/open-webui/pipelines
      enabled: false
      # -- This section can be used to pass required environment variables to your pipelines (e.g. Langfuse hostname)
      extraEnvVars:
      - name: RAG_EMBEDDING_MODEL_AUTO_UPDATE
        value: "False"
      - name: LITELLM_LOCAL_MODEL_COST_MAP
        value: "True"
      tika:
        # -- Automatically install Apache Tika to extend Open WebUI
        enabled: false

    # -- A list of Ollama API endpoints. These can be added in lieu of automatically installing the Ollama Helm chart, or in addition to it.
    ollamaUrls:
      - http://ollama-ipex.ai.svc.cluster.local:11434

    websocket:
      # -- Enables websocket support in Open WebUI with env `ENABLE_WEBSOCKET_SUPPORT`
      enabled: false

    # -- Value of cluster domain
    clusterDomain: uhl.cool

    annotations: {}
    podAnnotations: {}
    replicaCount: 1
    # -- Open WebUI image tags can be found here: https://github.com/open-webui/open-webui/pkgs/container/open-webui
    image:
      repository: ghcr.io/open-webui/open-webui
      tag: v0.6.22
      pullPolicy: "IfNotPresent"
    resources: {}
    service:
      type: LoadBalancer
      annotations: {}
      port: 80
      containerPort: 8080
      nodePort: ""
      labels: {}
      loadBalancerClass: ""
    ingress:
      enabled: true
      class: external
      # -- Use appropriate annotations for your Ingress controller, e.g., for NGINX:
      # nginx.ingress.kubernetes.io/rewrite-target: /
      host: &host "open-webui.${SECRET_DOMAIN}"
      tls: true
      existingSecret: ""
      annotations:
        external-dns.alpha.kubernetes.io/target: "external.${SECRET_DOMAIN}"
        nginx.ingress.kubernetes.io/enable-websocket: "true"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: "Open WebUI"
        gethomepage.dev/description: "Web UI for Ollama"
        gethomepage.dev/group: "AI"
        gethomepage.dev/icon: "open-webui.png"
      labels:
        gethomepage.dev/enabled: "true"
    persistence:
      enabled: true
      existingClaim: open-webui

    # -- Node labels for pod assignment.
    nodeSelector: {}

    # -- Tolerations for pod assignment
    tolerations: []

    # -- Affinity for pod assignment
    affinity: {}

    # -- OpenAI base API URL to use. Defaults to the Pipelines service endpoint when Pipelines are enabled, and "https://api.openai.com/v1" if Pipelines are not enabled and this value is blank
    openaiBaseApiUrl: ""

    # -- Additional environments variables on the output Deployment definition. Most up-to-date environment variables can be found here: https://docs.openwebui.com/getting-started/env-configuration/
    extraEnvVars:
      # -- Default API key value for Pipelines. Should be updated in a production deployment, or be changed to the required API key if not using Pipelines
      # - name: OLLAMA_BASE_URLS
      #  value: http://open-webui-ollama:11434
#      - name: USER_AGENT
#        value: "Open WebUI"
      - name: WEBUI_URL
        value: *host
      - name: GLOBAL_LOG_LEVEL
        value: "DEBUG"
      # valueFrom:
      #   secretKeyRef:
      #     name: pipelines-api-key
      #     key: api-key
      # - name: OPENAI_API_KEY
      #   valueFrom:
      #     secretKeyRef:
      #       name: openai-api-key
      #       key: api-key
      # - name: OLLAMA_DEBUG
      #   value: "1"

    # -- Configure pod security context
    # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-containe>
    podSecurityContext:
      {}
      # fsGroupChangePolicy: Always
      # sysctls: []
      # supplementalGroups: []
      # fsGroup: 1001

    # -- Configure container security context
    # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-containe>
    containerSecurityContext:
      {}
      # runAsUser: 1001
      # runAsGroup: 1001
      # runAsNonRoot: true
      # privileged: false
      # allowPrivilegeEscalation: false
      # readOnlyRootFilesystem: false
      # capabilities:
      #   drop:
      #     - ALL
      # seccompProfile:
      #   type: "RuntimeDefault"
