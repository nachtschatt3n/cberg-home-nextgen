apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai
data:
  config.yaml: |
    model_list:
      # Ollama Reason instance models (via OpenAI-compatible endpoint)
      # o3 and gpt-4.1 are available through OpenAI-compatible API
      - model_name: local-o3
        litellm_params:
          model: o3-2025-04-16
          api_base: http://192.168.30.111:11435/v1
          api_key: "ollama"  # Required but ignored by Ollama
      
      - model_name: local-gpt-4.1
        litellm_params:
          model: gpt-4.1-2025-04-14
          api_base: http://192.168.30.111:11435/v1
          api_key: "ollama"  # Required but ignored by Ollama
      
      # Standard Ollama models (via native Ollama API)
      - model_name: local-gpt-oss
        litellm_params:
          model: ollama/gpt-oss:20b
          api_base: http://192.168.30.111:11435
      
      # Ollama Vision instance models
      - model_name: local-qwen-vl
        litellm_params:
          model: ollama/qwen3-vl:8b-instruct
          api_base: http://192.168.30.111:11436
      
      # Ollama Voice instance models
      - model_name: local-qwen-voice
        litellm_params:
          model: ollama/qwen3:4b-instruct
          api_base: http://192.168.30.111:11434
    
    general_settings:
      # Master key for LiteLLM (can be any value, used for authentication)
      master_key: "bytebot-litellm-key-2025"
      
    router_settings:
      # Enable load balancing across models
      routing_strategy: "least-busy"
      
      # Fallback configuration
      model_group_alias:
        "smart-model": ["local-o3", "local-gpt-4.1"]
        "vision-model": ["local-qwen-vl"]
