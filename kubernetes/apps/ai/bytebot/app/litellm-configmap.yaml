apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai
data:
  config.yaml: |
    model_list:
      # Standard Ollama models (via native Ollama API)
      - model_name: local-gpt-oss
        litellm_params:
          model: ollama/gpt-oss:20b
          api_base: http://192.168.30.111:11435
      
      # Ollama Vision instance models
      - model_name: local-qwen-vl
        litellm_params:
          model: ollama/qwen3-vl:8b-instruct
          api_base: http://192.168.30.111:11436
      
      # Ollama Voice instance models
      - model_name: local-qwen-voice
        litellm_params:
          model: ollama/qwen3:4b-instruct
          api_base: http://192.168.30.111:11434
      
      - model_name: local-qwen-30b
        litellm_params:
          model: ollama/qwen3:30b-instruct
          api_base: http://192.168.30.111:11435
    
    general_settings:
      # Master key for LiteLLM (can be any value, used for authentication)
      master_key: "bytebot-litellm-key-2025"
      # Drop unsupported parameters for Ollama
      drop_params: true
      
    router_settings:
      # Enable load balancing across models
      routing_strategy: "least-busy"
      
      # Fallback configuration
      model_group_alias:
        "smart-model": ["local-gpt-oss", "local-qwen-30b"]
        "vision-model": ["local-qwen-vl"]
