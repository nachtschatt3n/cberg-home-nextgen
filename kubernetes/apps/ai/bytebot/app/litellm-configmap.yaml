apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai
data:
  config.yaml: |
    model_list:
      # Standard Ollama models (via native Ollama API)
      - model_name: local-gpt-oss
        litellm_params:
          model: ollama/gpt-oss:20b
          api_base: http://192.168.30.111:11435
      
      # Ollama Vision instance models
      - model_name: local-qwen-vl
        litellm_params:
          model: ollama/qwen3-vl:8b-instruct
          api_base: http://192.168.30.111:11436
      
      # Ollama Voice instance models
      - model_name: local-qwen-voice
        litellm_params:
          model: ollama/qwen3:4b-instruct
          api_base: http://192.168.30.111:11434
      
      - model_name: local-qwen-30b
        litellm_params:
          model: ollama/qwen3:30b-instruct
          api_base: http://192.168.30.111:11435
    
    general_settings:
      # Drop unsupported parameters for Ollama
      drop_params: true
      # Note: No master_key set - allows unauthenticated access
      # Bytebot will use OPENAI_API_KEY for chat requests, but /v1/models can be accessed without auth
      
    router_settings:
      # Enable load balancing across models
      routing_strategy: "least-busy"
      
      # Fallback configuration
      model_group_alias:
        "smart-model": ["local-gpt-oss", "local-qwen-30b"]
        "vision-model": ["local-qwen-vl"]
