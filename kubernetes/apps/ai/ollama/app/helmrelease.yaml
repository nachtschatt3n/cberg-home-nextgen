---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama-ipex
  namespace: ai
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 1
  install:
    remediation:
      retries: 1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 1
  uninstall:
    keepHistory: false
  dependsOn:
    - name: intel-device-plugin-gpu
      namespace: kube-system
  values:
    controllers:
      main:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          main:
            image:
              repository: ubuntu
              tag: 24.04
            env:
              OLLAMA_HOST: "0.0.0.0:11434"
              DEBIAN_FRONTEND: "noninteractive"
              TZ: "Europe/Berlin"
              ONEAPI_DEVICE_SELECTOR: "level_zero:0"
              IPEX_LLM_NUM_CTX: "16384"
              ZES_ENABLE_SYSMAN: "1"
              ZES_ENABLE_SYSMAN_DEVICE: "0"
              GIN_MODE: "release"
              LANG: "en_US.UTF-8"
            command:
              - "/bin/bash"
              - "-c"
              - |
                apt update
                apt install --no-install-recommends -q -y software-properties-common ca-certificates wget ocl-icd-libopencl1 zsh vim curl git coreutils sed build-essential gcc-11 g++-11 lowdown rocm*
                if [ -f "/root/.downloads/downloads.done" ]; then
                  echo "Download already completed. Prepare for Ollama..."
                  cd /root/.downloads
                else
                  echo "Downloading Intel GPU dependencies..."
                  mkdir -p /root/.downloads
                  cd /root/.downloads
                  wget https://github.com/oneapi-src/level-zero/releases/download/v1.21.9/level-zero_1.21.9+u24.04_amd64.deb
                  wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.8.3/intel-igc-core-2_2.8.3+18762_amd64.deb
                  wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.8.3/intel-igc-opencl-2_2.8.3+18762_amd64.deb
                  wget https://github.com/intel/compute-runtime/releases/download/25.09.32961.7/intel-level-zero-gpu_1.6.32961.7_amd64.deb
                  wget https://github.com/intel/compute-runtime/releases/download/25.09.32961.7/intel-opencl-icd_25.09.32961.7_amd64.deb
                  wget https://github.com/intel/compute-runtime/releases/download/25.09.32961.7/libigdgmm12_22.6.0_amd64.deb
                  wget https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz
                  git clone https://github.com/aristocratos/btop.git
                  touch /root/.downloads/downloads.done
                fi
                  echo "Installing Intel GPU dependencies..."
                  dpkg -i *.deb
                  mkdir -p /usr/local
                  tar xvf ollama-ipex-llm-2.3.0b20250415-ubuntu.tgz -C /usr/local
                  cd btop
                  make
                  make install
                  cd ..
                  echo "Intel GPU setup complete. Starting Ollama..."
                  exec /usr/local/ollama-ipex-llm-2.3.0b20250415-ubuntu/ollama serve

            probes:
              liveness:
                enabled: true
                type: AUTO
                port: 11434
                spec:
                  initialDelaySeconds: 50
                  periodSeconds: 30
                  timeoutSeconds: 5
                  failureThreshold: 5
              readiness:
                enabled: true
                type: AUTO
                port: 11434
                spec:
                  initialDelaySeconds: 50
                  periodSeconds: 30
                  timeoutSeconds: 5
                  failureThreshold: 5
            securityContext:
              privileged: true
              capabilities:
                add:
                  - SYS_ADMIN
              allowPrivilegeEscalation: true
              runAsUser: 0
            resources:
              requests:
                cpu: 500m
                memory: 32Gi
                gpu.intel.com/i915: 1
              limits:
                memory: 32Gi
                gpu.intel.com/i915: 1
    service:
      main:
        controller: main
        ports:
          http:
            port: 11434
    persistence:
      config:
        enabled: true
        existingClaim: ollama-ipex-config
        globalMounts:
          - path: /root/.ollama
      temp:
        enabled: true
        existingClaim: ollama-ipex-tmp
        globalMounts:
          - path: /root/.downloads

    hostNetwork: false
    securityContext:
      privileged: true
      capabilities:
        add:
          - SYS_ADMIN
      allowPrivilegeEscalation: true
      runAsUser: 0
      runAsGroup: 0
      fsGroup: 0
    podSecurityContext:
      hostPID: true
      hostIPC: true



# #wget https://github.com/mattcurf/ollama-intel-gpu/releases/download/v0.0.1/ollama-0.5.4-ipex-llm-2.2.0b20250220-ubuntu.tgz
#exec /usr/local/ollama-0.5.4-ipex-llm-2.2.0b20250220-ubuntu/ollama serve