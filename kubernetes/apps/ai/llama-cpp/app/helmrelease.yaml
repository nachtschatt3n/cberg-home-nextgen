---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app llama-cpp
  namespace: ai
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.6.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  maxHistory: 1
  install:
    remediation:
      retries: 1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 1
  uninstall:
    keepHistory: false
  dependsOn:
    - name: intel-device-plugin-gpu
      namespace: kube-system
  values:
    controllers:
      main:
        type: statefulset
        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          main:
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: full-intel
            env:
              TZ: "Europe/Berlin"
              # ZES_ENABLE_SYSMAN: "1"
              # GGML_SYCL_DISABLE_GRAPH: "1"
              # GGML_SYCL_DISABLE_OPT: "0"
              # ONEAPI_DEVICE_SELECTOR: "level_zero:0"
              # GGML_SYCL_DEBUG: "0"
              # SYCL_DEVICE_FILTER: "level_zero:gpu:0"
              # INTEL_DEVICE_SELECTOR: "level_zero:gpu:0"
              # GGML_SYCL_PRIORITIZE_DMMV: "0"
              # GGML_SYCL_DISABLE_DNN: "1"
              # OMP_NUM_THREADS: "8"
              # MKL_NUM_THREADS: "8"
              # exec /app/llama-server --host 0.0.0.0 --port 8080 --ctx-size 8192 --threads 8 --n-gpu-layers 35 --batch-size 1024 --ubatch-size 512 -hf unsloth/Llama-3.2-3B-Instruct-GGUF
            command:
              - "/bin/bash"
              - "-c"
              - |
                echo "Starting llama.cpp server with Intel GPU support..."
                exec /app/llama-server --host 0.0.0.0 --port 8080 -hf ggml-org/gemma-3-1b-it-GGUF

            probes:
              liveness:
                enabled: true
                type: AUTO
                port: 8080
                spec:
                  initialDelaySeconds: 60
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 5
              readiness:
                enabled: true
                type: AUTO
                port: 8080
                spec:
                  initialDelaySeconds: 60
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 5
            securityContext:
              privileged: true
              capabilities:
                add:
                  - SYS_ADMIN
              allowPrivilegeEscalation: true
              runAsUser: 0
            resources:
              requests:
                cpu: 500m
                memory: 8Gi
                gpu.intel.com/i915: 1
              limits:
                memory: 16Gi
                gpu.intel.com/i915: 1
    service:
      main:
        controller: main
        ports:
          http:
            port: 8080
    ingress:
      main:
        enabled: true
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/name: "Llama.cpp Server"
          gethomepage.dev/description: "Intel GPU-accelerated llama.cpp inference server"
          gethomepage.dev/group: "AI"
          gethomepage.dev/icon: "llama.png"
        hosts:
          - host: "llama-cpp.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
                service:
                  name: llama-cpp
                  port: 8080
        tls:
          - secretName: llama-cpp-tls
            hosts:
              - "llama-cpp.${SECRET_DOMAIN}"
    persistence:
      config:
        enabled: true
        existingClaim: llama-cpp-config
        globalMounts:
          - path: /root/.cache/llama.cpp
      models:
        enabled: true
        existingClaim: llama-cpp-models
        globalMounts:
          - path: /models
    hostNetwork: false
    securityContext:
      privileged: true
      capabilities:
        add:
          - SYS_ADMIN
      allowPrivilegeEscalation: true
      runAsUser: 0
      runAsGroup: 0
      fsGroup: 0
    podSecurityContext:
      hostPID: true
      hostIPC: true
