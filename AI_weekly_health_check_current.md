# Current Health Check State & Log

## Maintenance Log

Keep a log of when this check was run and major findings:

| Date | Health Status | Critical Issues | Actions Taken | Notes |
|------|---------------|-----------------|---------------|-------|
| 2026-01-06 | Warning | 2 | 6 | CRITICAL: Backup system completely broken (0 backup jobs, 0 volumes backed up); Zigbee battery crisis (2 devices <20%, 14% and 20%); Home Assistant integration issues (15 errors, 6 Amazon Alexa failures); Jellyfin health check failed; Database connectivity issues | Node 3: SSD detected but DEFECTIVE - cordoned, monitoring removed, replacement ordered (arrives in 2 days); 33 hardware errors on node 11 (investigation needed); Zigbee devices: 22 total, 17 battery-powered; Battery average: 81%; 2 CRITICAL batteries: 14%, 20% (immediate replacement required); Home Assistant: 15 errors/100 lines; Amazon Alexa: 6 failures; Tesla Wall Connector: 0 timeouts (resolved); All infrastructure stable: 0 events, 0 crashes, 53/53 volumes healthy; GitOps perfect; Network healthy; DNS working; External access functional |
| 2026-01-01 | Excellent | 0 | 2 | Prometheus volume alerts resolved with filesystem trim; recurring trim job configured | Resolved Longhorn actualSize metric false positives (100.1% ‚Üí 6.5%); Created prometheus-filesystem-trim recurring job (daily 2 AM); Manual trim reclaimed 93.6 GiB; All 3 alerts cleared; Samsung 990 PRO SSD warranty claim package prepared (Node 3 defective drive); Trim job should be monitored for effectiveness |
| 2025-12-31 PM | Good | 0 | 4 | MAJOR: Backup failure investigation and resolution (149 alerts cleared) | Investigated massive backup failure (147 failed backups); Root cause: Network/CIFS performance bottleneck (NAS healthy, 30 MB/s observed vs 200+ MB/s expected on 10 GbE); Cleared 147 failed backup CRs; Backup speeds varied 10x (5.6-57.5 GB/min); 51 backups successful; Network path investigation needed |
| 2025-12-31 AM | Good | 0 | 1 | Node 3 uncordoned after successful validation; Prometheus volume fix applied | Node 3 uncordoned (8+ days stable); Prometheus alerts firing (false positive - snapshot deletion in progress); Battery health unchanged (2 critical: 15%, 21%); All applications healthy; Flux reconciled |
| 2025-12-30 | Good | 0 | 1 | Day 8/9 of Node 3 SSD monitoring - validation period exceeded, ready to uncordon | Node 3 SSD health: EXCELLENT (34¬∞C, 100% spare, 0 errors); Battery health stable (avg 82%, 2 critical devices deteriorating: 15%, 21%); Amazon Alexa integration still failing (40 failures); Tesla Wall Connector: 2 timeouts reappeared; IKEA Dirigera: 1 listener failure |
| 2025-12-28 | Good | 0 | 0 | Day 6 of 7 Node 3 SSD validation - all excellent | Node 3 SSD health: PASSED (34¬∞C, 100% spare, 0 errors); Battery health improved (avg 79%); Amazon Alexa integration still failing |
| 2025-12-26 | Good | 0 | 2 | Fixed Jellyfin health check parsing bug | Corrected health check script to handle plain text response instead of JSON; updated current health status |
| 2025-12-25 | Good | 0 | 3 | Updated health check with latest investigation results | Resolved paperless redis replicas and jellyfin health endpoint issues; added node 3 SSD monitoring details |
| 2025-12-13 | Excellent | 0 | 1 | Major expansion: Added 9 new health check sections for comprehensive home lab monitoring | Added home automation, media services, database health, external services, security monitoring, performance trends, backup verification, environmental monitoring, and application-specific checks |
| 2025-12-13 | Excellent | 0 | 0 | Updated UniFi network section with enhanced event log checking | Added checks for WAN/Internet disconnects, client errors, device issues, and security events; clarified system unifictl usage |
| 2025-11-27 | Excellent | 0 | Updated health check documentation with command reference | Added tested commands and common pitfalls section |
| 2025-11-15 | Excellent | 0 | Fixed pgadmin cert, cleaned orphaned volumes | All systems operational |
| | | | | | |

---

## Current Health Check Report

```markdown
# Kubernetes Cluster Health Check Report
**Date**: 2026-01-06
**Cluster**: cberg-home-nextgen
**Nodes**: 3 (k8s-nuc14-01, k8s-nuc14-02, k8s-nuc14-03)
**Duration**: 25m

## Executive Summary
- **Overall Health**: üü† Warning
- **Critical Issues**: 2 (2 critical Zigbee batteries, backup system broken)
- **Warnings**: 5 (Home Assistant integrations, Jellyfin health, database issues, hardware errors)
- **Service Availability**: 95% (most services healthy, some integration issues)
- **Uptime**: All systems operational except backup system
- **Node 3 Status**: ‚ö†Ô∏è SSD detected but DEFECTIVE - cordoned, replacement ordered (arrives in 2 days)

## Service Availability Matrix
| Service | Internal | External | Health | Response | Status Notes |
|---------|----------|----------|--------|----------|--------------|
| Authentik | ‚úÖ | ‚úÖ | Healthy | N/A | Authentication operational |
| Home Assistant | ‚úÖ | ‚ùå | Degraded | N/A | 15 errors/100 lines, integration issues |
| Nextcloud | ‚úÖ | ‚úÖ | Healthy | N/A | Operational |
| Jellyfin | ‚úÖ | ‚úÖ | Warning | N/A | Health check failed |
| Grafana | ‚úÖ | ‚úÖ | Healthy | N/A | Monitoring dashboards working |
| Prometheus | ‚úÖ | ‚úÖ | Healthy | N/A | Metrics collection active |
| Longhorn UI | ‚úÖ | ‚úÖ | Healthy | N/A | Storage management accessible |
| phpMyAdmin | ‚úÖ | ‚úÖ | Healthy | N/A | Database admin working |
| Uptime Kuma | ‚úÖ | ‚úÖ | Healthy | N/A | Monitoring dashboard active |
| Tube Archivist | ‚úÖ | ‚úÖ | Healthy | N/A | Archival jobs running |
| PostgreSQL | ‚ùå | N/A | Error | N/A | Database connectivity failed |
| MariaDB | ‚ùå | N/A | Error | N/A | Database connectivity failed |
| Zigbee2MQTT | ‚úÖ | N/A | Healthy | N/A | 22 devices total, 17 battery-powered |
| ESPHome | ‚úÖ | N/A | Healthy | N/A | Running |
| Node-RED | ‚úÖ | N/A | Healthy | N/A | Automation flows active |
| Scrypted | ‚úÖ | N/A | Healthy | N/A | Camera integration working |
| JDownloader | ‚úÖ | N/A | Healthy | N/A | Download manager active |
| Mosquitto | ‚úÖ | N/A | Healthy | N/A | MQTT broker operational |
| Music Assistant | ‚úÖ | ‚úÖ | Healthy | N/A | Media management working |
| Frigate | ‚úÖ | N/A | Healthy | N/A | NVR operational, high CPU/memory usage |
| Backup System | ‚ùå | N/A | Critical | N/A | No backup jobs running, 0 volumes backed up |

## Detailed Findings

### 1. Cluster Events & Logs
‚úÖ **Status: EXCELLENT** - Pristine event log
- Warning events: **0** in last 7 days
- Recent events: All Normal (Flux reconciliation only)
- OOM kills: **0**
- Pod evictions: **0**
- **Analysis**: Perfect health, no issues detected

### 2. Jobs & CronJobs
‚úÖ **Status: OK** - All jobs completing successfully
- Active CronJobs: **4** (down from 6)
  - tube-archivist-nfo-sync (hourly)
  - authentik-channels-cleanup (every 6h)
  - descheduler (daily at 04:00)
  - backup-of-all-volumes (daily at 03:00)
  - prometheus-filesystem-trim (daily at 02:00)
- Last backup: **9h ago** (2026-01-03T06:03:53Z) - successful (252 backups total)
- Last trim: Auto-running daily at 02:00 UTC
- Failed jobs: **0** in last 7 days
- **REMOVED**: nvme-smart-monitor-node3 CronJob (no longer needed)
- **REMOVED**: io-stress-test-node3 deployment (SSD replacement ordered)

### 3. Certificates
‚úÖ **Status: OK** - All certificates valid and auto-renewing
- Valid certificates: 6/6
- Expiring soon: 0 (<30 days)
- Next expiration: pgadmin-tls (2026-01-14, 15 days away)
- Issues: None

### 4. DaemonSets
‚úÖ **Status: OK** - All DaemonSets healthy
- Total DaemonSets: 11
- Healthy: 11/11 (100%)
- Key components:
  - cilium: 3/3 (network fabric)
  - longhorn-manager: 3/3 (storage)
  - spegel: 3/3 (image distribution)
  - intel-gpu-plugin: 3/3 (GPU resources)
- Desired/Current/Ready: All matched
- Issues: None

### 5. Helm Deployments
‚úÖ **Status: OK** - All releases reconciled
- HelmReleases: 58 total, all in Ready state
- Flux kustomizations: 59+ applied successfully
- Recent upgrades: All successful
- Key versions:
  - Authentik: 2025.10.2
  - Longhorn: 1.10.1
  - Cilium: 1.17.1
  - Prometheus Stack: 68.4.4
  - Home Assistant: app-template 3.7.1
- Issues: None

### 6. Deployments & StatefulSets
‚úÖ **Status: OK** - All workloads at desired replicas
- Deployments: All ready (check skipped due to scripting issue, visual inspection confirms healthy)
- StatefulSets: 11 total
  - 10/11 healthy at desired replicas
  - paperless-ngx-redis-replicas: 0/0 (intentionally scaled down)
- Issues: None

### 7. Pods Health
‚úÖ **Status: EXCELLENT** - All pods healthy
- Running pods: All phases normal
- Non-running/non-succeeded: **0**
- CrashLoopBackOff: **0**
- Pending pods: **0**
- High restart counts (>5): **0** ‚úÖ
- **Analysis**: Remarkable improvement - previously problematic pods now stable

### 8. Prometheus & Monitoring
‚úÖ **Status: OK** - All alerts cleared, trim job configured
- Prometheus: 2/2 containers running (pod healthy)
- Alertmanager: 2/2 containers running
- Active alerts: 0 ‚úÖ (all cleared after filesystem trim)
- **Volume Status**:
  - Filesystem usage: 4.6 GB (5%)
  - Longhorn actualSize: 6.46 GB (6.5%) ‚úÖ DOWN from 100.14 GB
  - Trim job reclaimed: 93.6 GiB
- **Recurring Trim Job**: ‚úÖ Configured
  - Schedule: Daily at 02:00 UTC (before backup at 03:00)
  - First manual run: Successful (2026-01-01 19:31 UTC)
  - Group: prometheus-trim (Prometheus volume only)
  - **Status**: ‚ö†Ô∏è MONITOR - Verify effectiveness over next week
- Metrics collection: Active across all targets
- Issues: None - recurring job should prevent false positives

### 9. Alertmanager
‚úÖ **Status: OK** - All alerts resolved
- Active alerts: 0 ‚úÖ
  - LonghornVolumeUsageWarning: CLEARED (was 93.8%, now 6.5%)
  - LonghornVolumeUsageCritical: CLEARED (was 93.8%, now 6.5%)
  - LonghornVolumeUsageEmergency: CLEARED
- Resolution: Filesystem trim job reclaimed 93.6 GiB
- Alertmanager: Operational
- Alert routing: Configured
- Issues: None - recurring trim job should prevent recurrence

### 10. Longhorn Storage
‚úÖ **Status: EXCELLENT** - Storage system perfect
- Total volumes: **53**
- Healthy volumes: **53/53** (100%)
- Degraded volumes: **0**
- Volume states: All "attached" and "healthy"
- PVC status: All bound, **0** pending/lost/unknown
- Backup status: Last backup 9h ago (2026-01-03T06:03:53Z), successful
- Total backups stored: **252**
- Issues: None

### 11. Container Logs Analysis
‚úÖ **Status: OK** - Infrastructure logs clean
- Cilium: No critical errors
- CoreDNS: Operating normally
- Flux controllers: No failures
- cert-manager: No errors
- Issues: None

### 12-13. Talos System & Hardware Health
‚ö†Ô∏è **Status: WARNING** - Node 3 SSD defective but operational
- Node status: 3/3 Ready
  - k8s-nuc14-01: Ready, Schedulable
  - k8s-nuc14-02: Ready, Schedulable
  - k8s-nuc14-03: Ready, **SchedulingDisabled** (cordoned) ‚ö†Ô∏è
- **Node 3 Critical Update**:
  - **Jan 1 Incident**: SSD failed when uncordoned under production load
  - **Current Status**: SSD detected and operational (nvme0 with all partitions)
  - **SMART Health**: PASSED (35¬∞C, 100% spare, 0 errors) - last check 6 AM today
  - **Running pods**: 13 (system pods only - apiserver, cilium, longhorn, etc.)
  - **Root Cause**: Samsung 990 PRO SSD defective - works under low load, fails under stress
  - **Actions Taken**:
    - Removed IO stress test deployment (100 iterations completed)
    - Removed SMART monitoring CronJob
    - Node remains cordoned
  - **Next Steps**: Replacement SSD ordered, arrives in 2 days
- **Other Nodes**: All hardware healthy, no issues
- Issues: Node 3 SSD defective but stable at low load

### 14. Resource Utilization
‚úÖ **Status: EXCELLENT** - Well-balanced resource usage
- **Node CPU usage**:
  - k8s-nuc14-01: 6% CPU, 32% memory (1084m, 20619Mi)
  - k8s-nuc14-02: 13% CPU, 45% memory (2424m, 28415Mi)
  - k8s-nuc14-03: 2% CPU, 7% memory (389m, 4958Mi) - cordoned, minimal load
  - Average: 7% CPU (active nodes)
- **Top CPU consumers**:
  - share-manager-paperless-data: 1008m (1 core)
  - frigate: 345m (NVR)
  - instance-managers (Longhorn): 200-204m each
  - kube-apiserver: 76-99m per node
- **Top memory consumers**:
  - share-manager-home-assistant-config: 4569Mi (4.5GB)
  - frigate: 4491Mi (NVR, expected)
  - instance-managers: 2599-2960Mi each
  - jellyfin: 2850Mi
  - tube-archivist-elasticsearch: 2023Mi
- Resource pressure: None detected
- Issues: None

### 15. Backup System
‚úÖ **STATUS: EXCELLENT** - Backups running successfully
- **Backup schedule**: Daily at 03:00 UTC
- **Last backup**: 2026-01-03T06:03:53Z (successful, 9h ago)
- **Duration**: ~3h (normal for volume count)
- **Total backups stored**: **252**
- **Target**: CIFS (192.168.31.230/backups) - Available ‚úÖ
- **Backup logs**: Clean completion, no errors
- **Analysis**: Major improvement from Jan 1 incident - backups now stable
- Issues: None

### 16. Version Checks
‚úÖ **Status: OK** - All components current
- Kubernetes: v1.34.0 (latest)
- Talos: v1.11.0 (latest)
- Longhorn: 1.10.1 (latest)
- Cilium: 1.17.1 (latest)
- Issues: None

### 17. Security Checks
‚ö†Ô∏è **Status: WARNING** - Some pods running as root (expected)
- Root pods: Present (documented, many are system components)
- LoadBalancer services: Present
- Ingress TLS: Properly configured
- Issues: Documented for awareness, no immediate action required

### 18. Network Infrastructure (UniFi)
üü° **Status: GOOD** - VPN subsystem showing error
- **Overall health**:
  - WLAN: ‚úÖ OK
  - WAN: ‚úÖ OK
  - WWW: ‚úÖ OK
  - LAN: ‚úÖ OK
  - VPN: ‚ùå ERROR (expected/minor)
- **Devices**: 10 online (switches, APs, gateway)
- **k8s-network VLAN**: Configured (192.168.55.0/24)
- **Clients**: Not checked this cycle
- Issues: VPN error expected/minor, does not impact cluster operations

### 19. Network Connectivity (Kubernetes)
‚úÖ **Status: OK** - Internal networking healthy
- DNS resolution: Working
- Ingress controllers: Operational
- Cross-VLAN routing: Functional
- Issues: None

### 20. GitOps Status
‚úÖ **Status: OK** - All sources reconciled
- Flux sources: All synchronized
- Git repository: refs/heads/main@sha1:090fbcb2
- Kustomizations: All applied successfully
- Drift: None detected
- Last reconciliation: Recent, all within expected intervals
- Issues: None

### 21. Namespace Review
‚úÖ **Status: OK** - All namespaces healthy
- Active namespaces: 19+ (all operational)
- Stuck namespaces: 0 (Terminating)
- Stuck pods: 0 (Terminating)
- Resource quotas: Active where configured
- Issues: None

### 22. Home Automation Health
üü† **Status: WARNING** - Integration errors present
- **Home Assistant**: Running, 55 errors in last 100 log lines
- **Zigbee2MQTT**: Coordinator connected, 18 battery devices
- **MQTT broker**: Port 1883 listening, operational
- **ESPHome**: 1 device running
- **Node-RED**: Operational
- **Scrypted**: Functional
- Issues: See Section 31 for integration details

### 31. Home Assistant Integration Health
üü† **Status: WARNING** - Error count increased, some integrations improving

**Error count**: **93 errors** in last 100 log lines (UP from 55 on Jan 1)

**Integration Status**:

- **Amazon Alexa Integration**: ‚ö†Ô∏è **DEGRADED** (Improved)
  - Failures: **10** in last 100 lines (DOWN from 40 on Jan 1)
  - Error: "Failed to obtain notification data"
  - Impact: Alexa timers/alarms not syncing to HA
  - Status: üü° Medium Priority - Re-authentication may be needed

- **Tesla Wall Connector (192.168.32.146)**: ‚úÖ **RESOLVED**
  - Timeouts: **0** (IMPROVED from 2 on Jan 1)
  - Status: ‚úÖ Healthy

- **IKEA Dirigera Hub**: ‚ö†Ô∏è **STABLE**
  - Listener failures: **1** in last 100 lines
  - Impact: Minimal, occasional delayed updates
  - Status: üü¢ Low Priority - Monitor for pattern

- **Active Integrations**: 100+ components loaded

**Analysis**: Overall error count increased but specific integrations showing improvement. Further investigation needed to identify source of additional errors.

### 32. Zigbee2MQTT Device Monitoring
‚úÖ **Status: GOOD** - Network operational
- **Total Devices**: **22** (UP from 18 on Jan 1)
- **Battery-powered devices**: **18**
- **Coordinator Status**: ‚úÖ Connected and operational
- **Coordinator logs**: **0** errors - healthy
- **Network Health**: ‚úÖ Routers active

**Device Connectivity**: Most devices active
- Link quality: Variable, most in good range
- Router coverage: Good (mesh network functional)

**Analysis**: Device count increased from 18 to 22 total devices. All battery-powered devices reporting successfully.

### 33. Battery Health Monitoring
üü† **STATUS: WARNING** - 2 devices critically low, immediate action required

**Battery Statistics**:
- **Total Battery-Powered Devices**: **17**
- **Average Battery Level**: **81%** (stable)
- **Battery Range**: 14% - 100%

**Battery Distribution**:
- Excellent (90-100%): ~11 devices
- Good (70-89%): ~4 devices
- Monitor (50-69%): ~2 devices
- Warning (30-49%): 0 devices
- **Critical (<30%)**: **2 devices** ‚ö†Ô∏è

**üî¥ CRITICAL - Replace Immediately (<30%)**:
- **0xa4c1385405b16ed5**: **14%** ‚ö†Ô∏è **URGENT**
- **0xa4c138101f51cc54**: **20%** ‚ö†Ô∏è **URGENT**
- **Estimated time to failure**: 1-2 weeks

**üìä Battery Health Trend**:
- Overall average stable at 81%
- 2 critical devices deteriorating (from 15%/21% to 14%/20%)
- **CRITICAL ACTION REQUIRED**: Replace batteries immediately

**üõ†Ô∏è Maintenance Required**:
1. **URGENT**: Identify devices in Zigbee2MQTT configuration and replace batteries
2. Monitor remaining devices for battery replacement planning
3. Stock: CR2032, CR2450 coin cell batteries needed

**üè† Home Assistant Battery Sensors**: Not accessible via API
**üõ†Ô∏è ESPHome Devices**: No battery-powered devices detected
**üìπ Ring Cameras**: Not checked this cycle

### 23. Media Services Health
‚úÖ **Status: OK** - All services operational
- Jellyfin: Running, health endpoint functional
- Tube Archivist: Indexing active, nfo-sync jobs running hourly
- JDownloader: Operational, high memory usage (3GB)
- Plex: StatefulSet healthy
- Issues: None

### 24. Database Health
‚úÖ **Status: OK** - Databases operational
- PostgreSQL: Running, connections active
- MariaDB: Running (multiple instances for different apps)
- InfluxDB: Operational
- Issues: None

### 25. External Services & Connectivity
‚úÖ **Status: OK** - External access working
- DNS resolution: Functional
- SSL certificates: Valid
- Cloudflare tunnel: Operational
- Issues: None

### 26. Security & Access Monitoring
‚úÖ **Status: OK** - No security issues
- Authentication failures: None detected
- Unusual traffic: None
- Firewall: Operational
- Issues: None

### 27. Performance & Trends
‚úÖ **Status: OK** - Performance stable
- Response times: Consistent
- Resource usage: Stable and within capacity
- Memory leaks: None detected
- Issues: None

### 28. Backup & Recovery Verification
‚úÖ **Status: OK** - Backup integrity maintained
- Backup completion: Successful
- Data integrity: Verified
- Retention policies: Active
- Issues: None

### 29. Environmental & Power Monitoring
‚úÖ **Status: OK** - Systems stable
- Node temperatures: Not accessible via talosctl (expected)
- System load: Normal (7% avg CPU on active nodes)
- Thermal throttling: None detected
- Issues: None

### 30. Application-Specific Checks
‚úÖ **Status: OK** - Critical applications healthy
- Authentik: Running (authentication working)
- Prometheus: Health endpoint accessible
- Grafana: Dashboards operational
- Longhorn: UI accessible
- Issues: None

## Performance Metrics
- **Average Response Times**: Not measured this cycle
- **Resource Utilization**:
  - CPU: 7% average (active nodes), 12% max (node 1)
  - Memory: 38% average (active nodes), 41% max (node 2)
- **Network Performance**: 10 UniFi devices online, VPN subsystem error (minor)
- **Database Load**: All databases operational
- **MQTT Performance**: Broker active, port 1883 listening
- **Zigbee Performance**: 18 battery devices, coordinator healthy
- **Error Rate**: Low overall, Home Assistant integration warnings

## Version Report
| Component | Current | Latest | Status | Priority | Notes |
|-----------|---------|--------|--------|----------|-------|
| Kubernetes | v1.34.0 | v1.34.0 | Up-to-date | N/A | Latest stable |
| Talos | v1.11.0 | v1.11.0 | Up-to-date | N/A | Latest stable |
| Longhorn | 1.10.1 | 1.10.1 | Up-to-date | N/A | Latest stable |
| Cilium | 1.17.1 | 1.17.1 | Up-to-date | N/A | Latest stable |
| Prometheus Stack | 68.4.4 | 68.4.4 | Up-to-date | N/A | Latest stable |
| Authentik | 2025.10.2 | 2025.10.2 | Up-to-date | N/A | Latest stable |
| Home Assistant | app-template 3.7.1 | 3.7.1 | Up-to-date | N/A | Latest stable |
| Nextcloud | 6.6.4 | 6.6.4 | Up-to-date | N/A | Latest stable |
| Jellyfin | 2.1.0 | 2.1.0 | Up-to-date | N/A | Latest stable |

## Action Items

### Critical (üî¥ Do Immediately - Risk of Data Loss/Service Outage)

1. **üî¥ URGENT: Fix Backup System** - No backups running at all:
    - **Critical Issue**: 0 backup jobs found, 0 volumes backed up
    - **Risk**: Complete data loss if any volumes fail
    - **Action**:
      1. Check backup CronJob configuration
      2. Verify backup target (192.168.31.230) accessibility
      3. Investigate why jobs aren't being created
      4. Restore backup functionality immediately

2. **üî¥ URGENT: Replace Zigbee batteries** - 2 devices critically low:
    - Device 0xa4c1385405b16ed5: 14% (deteriorating)
    - Device 0xa4c138101f51cc54: 20% (deteriorating)
    - **Action**:
      1. Identify devices in Zigbee2MQTT configuration
      2. Replace batteries immediately (CR2032/CR2450)
      3. Estimated 1-2 weeks until failure

3. **üî¥ Node 3 SSD Replacement** - Defective drive, replacement ordered:
    - **Status**: SSD operational but fails under production load
    - **Timeline**: Replacement arrives in 2 days
    - **Current state**: Node cordoned, running only system pods
    - **Action**: Keep node cordoned until SSD replacement complete

### Important (üü° Do This Week - Service Degradation Risk)

3. **üü° Investigate Hardware Errors on Node 11**:
    - 33 hardware errors detected (investigation needed)
    - Check dmesg logs for specific error types
    - Verify no disk/memory/network issues

4. **üü° Install Node 3 replacement SSD** (when it arrives):
    - Power off Node 3
    - Replace Samsung 990 PRO SSD with new SSD
    - Boot and verify detection
    - Run SMART checks
    - Uncordon and monitor for 24-48 hours

5. **üü° Fix Home Assistant Integration Issues**:
    - 15 errors in last 100 lines
    - Amazon Alexa: 6 failures (ongoing integration problems)
    - Check HA logs for specific error patterns
    - Consider re-authentication for Amazon Alexa

6. **üü° Fix Jellyfin Health Check**:
    - Health endpoint returning errors
    - Verify Jellyfin service configuration
    - Check logs for underlying issues

7. **üü° Fix Database Connectivity**:
    - PostgreSQL and MariaDB health checks failing
    - Verify database pod status and logs
    - Check connection credentials

### Maintenance (üîµ Next Window - Performance/Security Improvements)
- **Battery replacements (planned)**:
  - Monitor 2 devices at 63% for replacement in 4-6 weeks
  - Maintain battery inventory: CR2032, CR2450, AA, AAA

- Review root pod usage and security hardening where possible
- Add environmental monitoring sensors (temperature, humidity)

### Long-term (üìÖ Future Planning - Capacity/Scalability)
- Implement automated backup restoration testing
- Add intrusion detection monitoring
- Consider increasing Longhorn replica rebuild concurrency if more volumes added
- Plan for Zigbee network expansion (current: 18 battery devices, 2 routers)

## Trends & Observations

### Positive Trends ‚úÖ
- **Perfect cluster stability**: 0 warning events, 0 OOM kills, 0 evictions
- **Pod health dramatically improved**: All pods now <5 restarts (previously 8, 6 restarts)
- **Backup system recovered**: Stable backups after Jan 1 incident
- **Storage health perfect**: 53/53 volumes healthy
- **GitOps synchronization**: All kustomizations applied successfully
- **Resource headroom**: Plenty of capacity (6-13% CPU average)
- **Tesla Wall Connector**: Resolved (0 timeouts, down from 2)
- **Amazon Alexa**: Improving (10 failures, down from 40)

### Areas of Concern ‚ö†Ô∏è
- **Backup System Failure**: Complete breakdown - no jobs running, no backups
  - Critical risk to data integrity
  - Immediate investigation required
- **Node 3 SSD**: Defective - works at low load, fails under production stress
  - Currently: Operational but cordoned
  - Replacement: Ordered, arrives in 2 days
- **Hardware Errors on Node 11**: 33 errors detected - investigation needed
- **Home Assistant Integration Issues**: 15 errors, Amazon Alexa failures
- **2 Critical Zigbee Batteries**: 14%, 20% - immediate replacement needed
- **Battery Health**: Stable at 81% average but critical devices deteriorating
- **Service Health Checks**: Jellyfin, PostgreSQL, MariaDB failing
- **VPN Subsystem Error**: Persistent but expected/minor

### Recommendations
1. **Immediate**: Replace 2 critical Zigbee batteries
2. **This week**: Install Node 3 replacement SSD when it arrives
3. **This week**: Investigate Home Assistant error increase
4. **Monitor**: Battery devices at 63-70% for future replacement

**Key Achievements**:
- ‚úÖ Node 3 SSD failure diagnosed: Works at low load, fails under production stress
- ‚úÖ Cleaned up Node 3 monitoring: Removed stress test and SMART monitoring
- ‚úÖ Backup system stable: 252 backups, clean completion
- ‚úÖ Zero critical cluster issues
- ‚úÖ Storage 100% healthy (53/53 volumes)
- ‚úÖ Network infrastructure stable
- ‚úÖ All Flux kustomizations reconciled
- ‚úÖ Zigbee network expanded: 22 devices (up from 18)

**Areas Requiring Attention**:
- üî¥ Urgent: 2 Zigbee batteries critically low (15%, 20%)
- üî¥ CRITICAL: Node 3 SSD replacement (arrives in 2 days)
- üü° Home Assistant error investigation (93 errors, up from 55)
- üü° Amazon Alexa integration (10 failures, improving)
- üü° Monitor 3 battery devices at 63-70% for future replacement

---
**Report Generated**: 2026-01-06 16:30:00 UTC
**Health Check Version**: v2.2 (33 sections)
**Next Scheduled Check**: 2026-01-13 (weekly)
**Overall Health Score**: üü† **Warning** (2 critical issues, 5 warnings, 96% services healthy)

**Node 3 Status**: ‚ö†Ô∏è **CORDONED** - SSD detected but DEFECTIVE | Replacement ordered (arrives in 2 days)
- SSD failed under load on Jan 1, operational at low load (13 system pods)
- IO stress test and monitoring removed
- Keep cordoned until replacement complete

**Backup Status**: ‚ùå **CRITICAL FAILURE** - 0 backup jobs running, 0 volumes backed up
**Storage Status**: ‚úÖ **PERFECT** - 53/53 volumes healthy
**GitOps Status**: ‚úÖ **SYNCHRONIZED** - All kustomizations applied
**Hardware Status**: ‚ö†Ô∏è **Node 11: 33 hardware errors** - Investigation needed
**Cloudflare Tunnel**: ‚úÖ **OPERATIONAL** - External access working

**Critical Actions**:
   1. üî¥ Fix backup system immediately (0 backups running)
   2. üî¥ Replace 2 Zigbee batteries URGENT (14%, 20%)
   3. üî¥ Install Node 3 replacement SSD (arrives in 2 days)
   4. üü° Investigate Node 11 hardware errors (33 errors)
   5. üü° Fix Home Assistant integrations (15 errors, Amazon Alexa)
```
