# Current Health Check State & Log

## Maintenance Log

Keep a log of when this check was run and major findings:

| Date | Health Status | Critical Issues | Actions Taken | Notes |
|------|---------------|-----------------|---------------|-------|
| 2026-01-17 | Warning | 1 | 2 | **Mixed**: UnPoller service fixed (metrics target UP in Prometheus, but metrics empty - user switched to InfluxDB); Pod evictions cleared (clawd-bot npm-cache limit increased); New Service created for UnPoller to fix missing target; **Known Issue**: UnPoller metrics checks failing due to InfluxDB switch; **Critical**: 2 Pod evictions detected (resolved by limit increase); Home Assistant errors: 30 (external integrations); Backup system healthy |
| 2026-01-10 PM | Excellent | 0 | 2 | **MAJOR IMPROVEMENTS**: Certificate conflict RESOLVED (adguard-home-tls now Ready via cert-manager ingress annotation); tube-archivist PVC reconciliation RESOLVED (manifest updated to 12Gi, kustomization Ready); All certificates Ready (5/5 = 100%); Backup system working (last backup 8h ago, 44/45 volumes backed up); All Prometheus alerts cleared (only Watchdog); **NEW ISSUE**: clawd-bot kustomization failing (missing secret.sops.yaml); clawd-bot-data volume detached but PVC bound; Home Assistant errors increased to 40/100 lines; Zigbee devices: 22 total (battery check needs investigation) |
| 2026-01-10 AM | Good | 1 | 0 | **EXCELLENT**: All Prometheus alerts cleared (0 firing); Home Assistant errors DOWN to 1 (from 2!); Backup system working (last backup 6h47m ago, completed successfully); All 3 nodes healthy; GitOps 59/60 reconciled (1 issue: tube-archivist PVC - manifest still shows 10Gi, needs update to 12Gi); Certificate conflict: adguard-home-tls STILL EXISTS (duplicate certificate not resolved); **NEW**: 1 unhealthy volume (clawd-bot-data: detached, unknown robustness - new volume created since yesterday); **CRITICAL**: Zigbee batteries still need replacement (12%, 18% - unchanged) |
| 2026-01-09 PM | Good | 1 | 0 | **EXCELLENT**: All Prometheus alerts cleared (0 firing); Home Assistant errors DOWN to 2 (from 11!); external-dns stable (813 restarts historical, now running fine); All 44 volumes healthy; All 3 nodes healthy; GitOps 59/60 reconciled (1 issue: tube-archivist PVC - manifest shows 10Gi but PVC is 12Gi, needs manifest update); Certificate conflict: adguard-home-tls (Helm chart creating duplicate certificate, needs disabling); Backup system: **WORKING** - Longhorn RecurringJob running successfully (last backup 17h ago, job completed); **CRITICAL**: Zigbee batteries still need replacement (12%, 18%) |
| 2026-01-09 PM | Good | 1 | 3 | FIXED: external-dns stabilized (penpot annotation added); tube-archivist volume expanded 10Giâ†’12Gi (94.3%â†’78.8%); adguard-home-tls certificate fixed (removed duplicate annotation); All Prometheus alerts cleared (except Watchdog) | CRITICAL: Zigbee batteries still need replacement (12%, 18%); Tesla Wall Connector timeouts are EXPECTED (power save mode when not charging); All services operational; external-dns running stable with 0 errors; Volume expansion successful; Certificate conflict resolved |
| 2026-01-09 AM | Warning | 1 | 6 | CRITICAL: Zigbee batteries deteriorating further (12%, 18% - down from 14%, 20%); 5 Prometheus alerts firing; Tesla Wall Connector regression (4 timeouts) | MAJOR IMPROVEMENTS: Backup system RESTORED (last backup 13h ago); Node 3 UNCORDONED and healthy; Home Assistant errors DOWN to 11 (from 93); Amazon Alexa RESOLVED (0 failures); All 44 volumes healthy; All 3 nodes at 5% CPU; GitOps synchronized; Cloudflare tunnel operational; Battery average: 80% (stable); WARNING: adguard-home-tls certificate not ready; external-dns CrashLoopBackOff is EXPECTED (Cloudflare proxy rejects private IPs, external access via tunnel works fine) |
| 2026-01-06 | Warning | 2 | 6 | CRITICAL: Backup system completely broken (0 backup jobs, 0 volumes backed up); Zigbee battery crisis (2 devices <20%, 14% and 20%); Home Assistant integration issues (15 errors, 6 Amazon Alexa failures); Jellyfin health check failed; Database connectivity issues | Node 3: SSD detected but DEFECTIVE - cordoned, monitoring removed, replacement ordered (arrives in 2 days); 33 hardware errors on node 11 (investigation needed); Zigbee devices: 22 total, 17 battery-powered; Battery average: 81%; 2 CRITICAL batteries: 14%, 20% (immediate replacement required); Home Assistant: 15 errors/100 lines; Amazon Alexa: 6 failures; Tesla Wall Connector: 0 timeouts (resolved); All infrastructure stable: 0 events, 0 crashes, 53/53 volumes healthy; GitOps perfect; Network healthy; DNS working; External access functional |
| 2026-01-01 | Excellent | 0 | 2 | Prometheus volume alerts resolved with filesystem trim; recurring trim job configured | Resolved Longhorn actualSize metric false positives (100.1% â†’ 6.5%); Created prometheus-filesystem-trim recurring job (daily 2 AM); Manual trim reclaimed 93.6 GiB; All 3 alerts cleared; Samsung 990 PRO SSD warranty claim package prepared (Node 3 defective drive); Trim job should be monitored for effectiveness |
| 2025-12-31 PM | Good | 0 | 4 | MAJOR: Backup failure investigation and resolution (149 alerts cleared) | Investigated massive backup failure (147 failed backups); Root cause: Network/CIFS performance bottleneck (NAS healthy, 30 MB/s observed vs 200+ MB/s expected on 10 GbE); Cleared 147 failed backup CRs; Backup speeds varied 10x (5.6-57.5 GB/min); 51 backups successful; Network path investigation needed |
| 2025-12-31 AM | Good | 0 | 1 | Node 3 uncordoned after successful validation; Prometheus volume fix applied | Node 3 uncordoned (8+ days stable); Prometheus alerts firing (false positive - snapshot deletion in progress); Battery health unchanged (2 critical: 15%, 21%); All applications healthy; Flux reconciled |
| 2025-12-30 | Good | 0 | 1 | Day 8/9 of Node 3 SSD monitoring - validation period exceeded, ready to uncordon | Node 3 SSD health: EXCELLENT (34Â°C, 100% spare, 0 errors); Battery health stable (avg 82%, 2 critical devices deteriorating: 15%, 21%); Amazon Alexa integration still failing (40 failures); Tesla Wall Connector: 2 timeouts reappeared; IKEA Dirigera: 1 listener failure |
| 2025-12-28 | Good | 0 | 0 | Day 6 of 7 Node 3 SSD validation - all excellent | Node 3 SSD health: PASSED (34Â°C, 100% spare, 0 errors); Battery health improved (avg 79%); Amazon Alexa integration failing |
| 2025-12-26 | Good | 0 | 2 | Fixed Jellyfin health check parsing bug | Corrected health check script to handle plain text response instead of JSON; updated current health status |
| 2025-12-25 | Good | 0 | 3 | Updated health check with latest investigation results | Resolved paperless redis replicas and jellyfin health endpoint issues; added node 3 SSD monitoring details |
| 2025-12-13 | Excellent | 0 | 1 | Major expansion: Added 9 new health check sections for comprehensive home lab monitoring | Added home automation, media services, database health, external services, security monitoring, performance trends, backup verification, environmental monitoring, and application-specific checks |
| 2025-12-13 | Excellent | 0 | 0 | Updated UniFi network section with enhanced event log checking | Added checks for WAN/Internet disconnects, client errors, device issues, and security events; clarified system unifictl usage |
| 2025-11-27 | Excellent | 0 | Updated health check documentation with command reference | Added tested commands and common pitfalls section |
| 2025-11-15 | Excellent | 0 | Fixed pgadmin cert, cleaned orphaned volumes | All systems operational |

---

## Current Health Check Report

```markdown
# Kubernetes Cluster Health Check Report
**Date**: 2026-01-17 19:18 CET
**Cluster**: cberg-home-nextgen
**Nodes**: 3 (k8s-nuc14-01, k8s-nuc14-02, k8s-nuc14-03)
**Duration**: N/A

## Executive Summary
- **Overall Health**: ðŸŸ  **Warning**
- **Critical Issues**: **1** (Pod evictions detected - resolved by limit increase)
- **Warnings**: 4 (UnPoller metrics missing, UnPoller errors, Home Assistant integration errors, High warning event count)
- **Service Availability**: 99% (most services healthy, UnPoller metrics unavailable in Prometheus)
- **Uptime**: All systems operational
- **Node Status**: âœ… **ALL 3 NODES HEALTHY**
- **Recent Changes**: UnPoller service created, clawd-bot volume limit increased, git-test pod removed

## Service Availability Matrix
| Service | Internal | External | Health | Response | Status Notes |
|---------|----------|----------|--------|----------|--------------|
| Authentik | âœ… | âœ… | Healthy | N/A | Authentication operational (6/6 pods ready) |
| Home Assistant | âœ… | âœ… | Warning | N/A | 30 errors/100 lines - external integrations failing |
| Nextcloud | âœ… | âœ… | Healthy | N/A | Operational |
| Jellyfin | âœ… | âœ… | Healthy | N/A | Running normally |
| Grafana | âœ… | âœ… | Healthy | N/A | Monitoring dashboards working |
| Prometheus | âœ… | âœ… | Healthy | N/A | 0 firing alerts |
| Alertmanager | âœ… | âœ… | Healthy | N/A | Operational |
| Longhorn UI | âœ… | âœ… | Healthy | N/A | Storage management accessible |
| UnPoller | âœ… | N/A | Warning | N/A | Service created, target UP, but metrics empty (User switched to InfluxDB) |
| Backup System | âœ… | N/A | Excellent | N/A | Last backup successful |

## Detailed Findings

### 1. UnPoller Status
âš ï¸ **Status: WARNING** - Metrics configuration change
- Service created: `unpoller` (ClusterIP, port 9130)
- Prometheus Target: **UP** (10.69.1.36:9130)
- Metrics: **Empty** in Prometheus
- **Root Cause**: User switched UnPoller to InfluxDB backend. Prometheus scraping is enabled but returning no device metrics.
- **Action**: Acknowledged configuration change.

### 2. Pod Evictions
ðŸ”´ **Status: CRITICAL (Resolved)** - Evictions detected
- Affected Pods: `clawd-bot`
- Reason: `npm-cache` volume limit exceeded (500Mi)
- **Resolution**: Volume limit increased to 2Gi in HelmRelease.
- Current Status: Eviction events visible in history, but new pods running correctly.

### 3. Home Assistant
âš ï¸ **Status: WARNING** - Integration errors
- Error count: 30 errors in last 100 lines
- Affected Integrations: Bermuda (BLE), Deebot, Shelly
- **Analysis**: External connectivity or device issues causing log noise. Core functionality appears intact.

### 4. GitOps Status
âœ… **Status: EXCELLENT** - Fully Synchronized
- Git sources: All reconciled
- Kustomizations: All reconciled
- HelmReleases: All Ready

## Action Items

### ðŸŸ¡ High Priority (Address Soon)
1. **UnPoller Metrics**
   - **Issue**: Health check expects Prometheus metrics, but UnPoller uses InfluxDB.
   - **Action**: Update health check script to skip Prometheus checks for UnPoller or query InfluxDB if possible.

2. **Home Assistant Integrations**
   - **Issue**: High error rate from external integrations.
   - **Action**: Investigate specific device connectivity (Shelly, Deebot).

### ðŸ”µ Medium Priority (Monitor)
1. **clawd-bot resource usage**
   - **Issue**: `npm-cache` usage growing.
   - **Action**: Monitor usage against new 2Gi limit.

## Summary

**Overall Health**: ðŸŸ  **Warning**

The cluster is stable with key services operational. The "Warning" status is primarily due to:
1.  **UnPoller configuration change**: Switched to InfluxDB, causing Prometheus-based checks to fail (expected).
2.  **Pod evictions**: Historical events from `clawd-bot` (resolved).
3.  **Home Assistant noise**: Integration errors in logs.

All critical infrastructure (Storage, Network, Backup, Nodes) is in **Excellent** condition.
```